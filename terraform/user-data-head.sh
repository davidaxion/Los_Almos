#!/bin/bash
set -e

# User Data Script for SLURM Head Node (Controller)
# This script runs on first boot to configure the SLURM controller

echo "========================================="
echo "Configuring SLURM Head Node (Controller)"
echo "========================================="

# Variables from Terraform
EFS_ID="${efs_id}"
S3_BUCKET="${s3_bucket}"
WORKER_COUNT="${worker_node_count}"

# Update system
apt-get update
apt-get upgrade -y

# Install essential packages
apt-get install -y \
    nfs-common \
    awscli \
    jq \
    htop \
    tmux \
    git \
    build-essential

# Mount EFS
echo "Mounting EFS: $EFS_ID"
mkdir -p /efs/models
echo "$EFS_ID:/ /efs/models nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 0 0" >> /etc/fstab
mount -a
chmod 755 /efs/models

# Install SLURM
echo "Installing SLURM..."
apt-get install -y \
    slurm-wlm \
    slurm-wlm-doc \
    munge

# Configure Munge (authentication for SLURM)
systemctl enable munge
dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key
chmod 400 /etc/munge/munge.key
chown munge:munge /etc/munge/munge.key
systemctl restart munge

# Get instance information
PRIVATE_IP=$(hostname -I | awk '{print $1}')
HOSTNAME=$(hostname -s)

# Create SLURM configuration
cat > /etc/slurm/slurm.conf <<EOF
# SLURM Configuration for Los Alamos Testing Cluster
# Auto-generated by Terraform

# Controller configuration
ClusterName=los-alamos-cluster
ControlMachine=$HOSTNAME
ControlAddr=$PRIVATE_IP

# Authentication
AuthType=auth/munge
CryptoType=crypto/munge

# Scheduling
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Logging
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

# State preservation
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d

# Timeouts
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# Job control
MaxJobCount=10000
MaxArraySize=1000

# GPU resource tracking
GresTypes=gpu

# Nodes configuration (will be updated dynamically)
NodeName=$HOSTNAME CPUs=4 RealMemory=15000 Gres=gpu:1 State=UNKNOWN
PartitionName=gpu Nodes=$HOSTNAME Default=YES MaxTime=INFINITE State=UP
EOF

# Create SLURM directories
mkdir -p /var/log/slurm /var/spool/slurm/ctld /var/spool/slurm/d
chown slurm:slurm /var/log/slurm /var/spool/slurm/ctld /var/spool/slurm/d

# Configure GPU resources
echo "NodeName=$HOSTNAME Name=gpu File=/dev/nvidia0" > /etc/slurm/gres.conf
chmod 644 /etc/slurm/gres.conf

# Install Python ML packages
echo "Installing Python ML packages..."
pip3 install --upgrade pip
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip3 install transformers accelerate vllm jupyterlab

# Install shared-model-utils
cd /opt
git clone https://github.com/anthropics/shared-model-utils.git || \
  echo "ModelLoader will be manually installed"
if [ -d "shared-model-utils" ]; then
  cd shared-model-utils
  pip3 install -e .
fi

# Create example SLURM job scripts
mkdir -p /opt/slurm/examples

cat > /opt/slurm/examples/test-gpu.sh <<'EOJOB'
#!/bin/bash
#SBATCH --job-name=gpu-test
#SBATCH --output=/tmp/gpu-test-%j.out
#SBATCH --error=/tmp/gpu-test-%j.err
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --time=00:10:00

echo "========================================="
echo "GPU Test Job - SLURM Job ID: $SLURM_JOB_ID"
echo "========================================="
echo "Node: $(hostname)"
echo "GPU Allocated: $CUDA_VISIBLE_DEVICES"
echo

nvidia-smi

echo
echo "Running PyTorch GPU test..."
python3 <<EOF
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    # Simple tensor operation
    x = torch.rand(1000, 1000).cuda()
    y = torch.rand(1000, 1000).cuda()
    z = torch.matmul(x, y)
    print(f"Matrix multiplication test: PASSED")
EOF

echo "========================================="
echo "GPU test completed successfully!"
echo "========================================="
EOJOB

chmod +x /opt/slurm/examples/test-gpu.sh

cat > /opt/slurm/examples/inference-vllm.sh <<'EOJOB'
#!/bin/bash
#SBATCH --job-name=vllm-inference
#SBATCH --output=/tmp/vllm-inference-%j.out
#SBATCH --error=/tmp/vllm-inference-%j.err
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --time=00:30:00

echo "========================================="
echo "vLLM Inference Job - SLURM Job ID: $SLURM_JOB_ID"
echo "========================================="

MODEL_PATH="/efs/models/gpt2"

if [ ! -d "$MODEL_PATH" ]; then
  echo "Model not found at $MODEL_PATH"
  echo "Available models:"
  ls -la /efs/models/
  exit 1
fi

python3 <<EOF
from vllm import LLM, SamplingParams

print("Loading model from $MODEL_PATH...")
llm = LLM(model="$MODEL_PATH", gpu_memory_utilization=0.8)

prompts = [
    "The future of AI is",
    "Machine learning enables us to"
]

sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"\nPrompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
EOF

echo "========================================="
echo "vLLM inference completed!"
echo "========================================="
EOJOB

chmod +x /opt/slurm/examples/inference-vllm.sh

# Start SLURM controller
systemctl enable slurmctld
systemctl start slurmctld

# Also run slurmd on head node for local jobs
systemctl enable slurmd
systemctl start slurmd

# Create welcome message
cat > /etc/motd <<'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                   Los Alamos SLURM GPU Testing Cluster                     ‚ïë
‚ïë                           HEAD NODE (Controller)                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìä SLURM Commands:
  sinfo              - View cluster status
  squeue             - View job queue
  sbatch <script>    - Submit batch job
  srun <command>     - Run interactive job
  scancel <jobid>    - Cancel job

üìÅ Storage:
  /efs/models/       - Shared model storage (EFS)
  S3 Bucket:         - ${S3_BUCKET}

üîß Example Jobs:
  /opt/slurm/examples/test-gpu.sh        - GPU functionality test
  /opt/slurm/examples/inference-vllm.sh  - vLLM inference example

üíª Jupyter Lab:
  jupyter lab --ip=0.0.0.0 --no-browser

üìö Documentation:
  https://slurm.schedmd.com/quickstart.html

EOF

echo "========================================="
echo "Head node configuration complete!"
echo "========================================="
echo "SLURM Controller: Running"
echo "EFS Mount: /efs/models"
echo "S3 Bucket: $S3_BUCKET"
echo "Example jobs: /opt/slurm/examples/"
echo "========================================="
